{
  
    
        "post0": {
            "title": "NLP -  Named Entity Recognition - Inference",
            "content": ". !pip install transformers . Collecting transformers Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.5MB 4.1MB/s Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0) Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12) Requirement already satisfied: importlib-metadata; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.0) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20) Collecting tokenizers&lt;0.11,&gt;=0.10.1 Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 25.0MB/s Collecting huggingface-hub==0.0.12 Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1) Collecting sacremoses Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 901kB 43.0MB/s Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9) Requirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (2021.5.30) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (1.24.3) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;transformers) (3.4.1) Requirement already satisfied: typing-extensions&gt;=3.6.4; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;transformers) (3.7.4.3) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers) (1.15.0) Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers) (7.1.2) Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers) (1.0.1) Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;transformers) (2.4.7) Installing collected packages: tokenizers, huggingface-hub, sacremoses, transformers Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2 . !pip install spacy -U . Collecting spacy Downloading https://files.pythonhosted.org/packages/c1/da/61f934c6ae177a291c77246ef91a78cab44a2d76f79e6892ca7b17571adf/spacy-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4MB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.4MB 5.1MB/s Collecting pathy&gt;=0.3.5 Downloading https://files.pythonhosted.org/packages/65/ae/ecfa3e2dc267010fa320034be0eb3a8e683dc98dae7e70f92b41605b4d35/pathy-0.6.0-py3-none-any.whl (42kB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51kB 8.4MB/s Requirement already satisfied, skipping upgrade: cymem&lt;2.1.0,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5) Requirement already satisfied, skipping upgrade: typing-extensions&lt;4.0.0.0,&gt;=3.7.4; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3) Requirement already satisfied, skipping upgrade: tqdm&lt;5.0.0,&gt;=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1) Collecting thinc&lt;8.1.0,&gt;=8.0.7 Downloading https://files.pythonhosted.org/packages/7a/6e/bd2da3d71ab2d175248949ac106fee09ae13bfaca39002eabdbd908b7440/thinc-8.0.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (619kB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 624kB 40.0MB/s Requirement already satisfied, skipping upgrade: murmurhash&lt;1.1.0,&gt;=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5) Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.0.0) Collecting catalogue&lt;2.1.0,&gt;=2.0.4 Downloading https://files.pythonhosted.org/packages/9c/10/dbc1203a4b1367c7b02fddf08cb2981d9aa3e688d398f587cea0ab9e3bec/catalogue-2.0.4-py3-none-any.whl Requirement already satisfied, skipping upgrade: wasabi&lt;1.1.0,&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2) Collecting pydantic!=1.8,!=1.8.1,&lt;1.9.0,&gt;=1.7.4 Downloading https://files.pythonhosted.org/packages/9f/f2/2d5425efe57f6c4e06cbe5e587c1fd16929dcf0eb90bd4d3d1e1c97d1151/pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1MB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.1MB 32.1MB/s Requirement already satisfied, skipping upgrade: numpy&gt;=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5) Requirement already satisfied, skipping upgrade: requests&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0) Collecting srsly&lt;3.0.0,&gt;=2.4.1 Downloading https://files.pythonhosted.org/packages/c3/84/dfdfc9f6f04f6b88207d96d9520b911e5fec0c67ff47a0dea31ab5429a1e/srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456kB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 460kB 38.7MB/s Collecting typer&lt;0.4.0,&gt;=0.3.0 Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl Requirement already satisfied, skipping upgrade: packaging&gt;=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (20.9) Requirement already satisfied, skipping upgrade: preshed&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5) Requirement already satisfied, skipping upgrade: blis&lt;0.8.0,&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1) Collecting spacy-legacy&lt;3.1.0,&gt;=3.0.7 Downloading https://files.pythonhosted.org/packages/d3/e8/1bc00eeff3faf1c50bde941f88a491a5c1128debb75dd8c913401e71585c/spacy_legacy-3.0.8-py2.py3-none-any.whl Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3) Requirement already satisfied, skipping upgrade: smart-open&lt;6.0.0,&gt;=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy&gt;=0.3.5-&gt;spacy) (5.1.0) Requirement already satisfied, skipping upgrade: zipp&gt;=0.5; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.7/dist-packages (from catalogue&lt;2.1.0,&gt;=2.0.4-&gt;spacy) (3.4.1) Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (1.24.3) Requirement already satisfied, skipping upgrade: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (2021.5.30) Requirement already satisfied, skipping upgrade: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (3.0.4) Requirement already satisfied, skipping upgrade: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (2.10) Requirement already satisfied, skipping upgrade: click&lt;7.2.0,&gt;=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer&lt;0.4.0,&gt;=0.3.0-&gt;spacy) (7.1.2) Requirement already satisfied, skipping upgrade: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging&gt;=20.0-&gt;spacy) (2.4.7) Requirement already satisfied, skipping upgrade: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2-&gt;spacy) (2.0.1) Installing collected packages: typer, pathy, pydantic, catalogue, srsly, thinc, spacy-legacy, spacy Found existing installation: catalogue 1.0.0 Uninstalling catalogue-1.0.0: Successfully uninstalled catalogue-1.0.0 Found existing installation: srsly 1.0.5 Uninstalling srsly-1.0.5: Successfully uninstalled srsly-1.0.5 Found existing installation: thinc 7.4.0 Uninstalling thinc-7.4.0: Successfully uninstalled thinc-7.4.0 Found existing installation: spacy 2.2.4 Uninstalling spacy-2.2.4: Successfully uninstalled spacy-2.2.4 Successfully installed catalogue-2.0.4 pathy-0.6.0 pydantic-1.8.2 spacy-3.1.0 spacy-legacy-3.0.8 srsly-2.4.1 thinc-8.0.7 typer-0.3.2 . from google.colab import drive drive.mount(&#39;/content/gdrive&#39;) . Mounted at /content/gdrive . !curl https://github.com/elenanereiss/Legal-Entity-Recognition/raw/master/data/dataset_courts.zip -L -o /content/raw.zip . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 168 100 168 0 0 1322 0 --:--:-- --:--:-- --:--:-- 1312 100 4289k 100 4289k 0 0 14.6M 0 --:--:-- --:--:-- --:--:-- 14.6M . ls /content/ . gdrive/ raw.zip sample_data/ . !unzip /content/raw.zip -d /content/raw/ . Archive: /content/raw.zip inflating: /content/raw/bfh.conll inflating: /content/raw/bgh.conll inflating: /content/raw/bpatg.conll inflating: /content/raw/bsg.conll inflating: /content/raw/bverfg.conll inflating: /content/raw/bverwg.conll inflating: /content/raw/bag.conll . from pathlib import Path import re def read_wnut(file_path): file_path = Path(file_path) raw_text = file_path.read_text().strip() raw_docs = re.split(r&#39; n t? n&#39;, raw_text) token_docs = [] tag_docs = [] for doc in raw_docs: tokens = [] tags = [] for line in doc.split(&#39; n&#39;): # print(line) token, tag = line.split() tokens.append(token) tags.append(tag) token_docs.append(tokens) tag_docs.append(tags) return token_docs, tag_docs train_texts, train_tags = read_wnut(&#39;/content/raw/bag.conll&#39;) val_texts, val_tags = read_wnut(&#39;/content/raw/bgh.conll&#39;) . tags = train_tags + val_tags . unique_tags = set(tag for doc in tags for tag in doc) tag2id = {tag: id for id, tag in enumerate(unique_tags)} id2tag = {id: tag for tag, id in tag2id.items()} . from transformers import Trainer, TrainingArguments training_args = TrainingArguments( output_dir=&#39;./results&#39;, # output directory num_train_epochs=10, # total number of training epochs per_device_train_batch_size=16, # batch size per device during training per_device_eval_batch_size=64, # batch size for evaluation warmup_steps=500, # number of warmup steps for learning rate scheduler weight_decay=0.01, # strength of weight decay logging_dir=&#39;./logs&#39;, # directory for storing logs logging_steps=1000, ) from transformers import AutoModelForTokenClassification model = AutoModelForTokenClassification.from_pretrained(&quot;/content/gdrive/MyDrive/NER_01_030_2021_07_08.bin&quot;, local_files_only=True, num_labels=len(unique_tags)) trainer = Trainer( model=model, # the instantiated ðŸ¤— Transformers model to be trained args=training_args, # training arguments, defined above # train_dataset=train_dataset, # training dataset # eval_dataset=val_dataset # evaluation dataset ) . import torch class COURTS_Dataset_inf(torch.utils.data.Dataset): def __init__(self, encodings): self.encodings = encodings def __getitem__(self, idx): item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} return item def __len__(self): return len(self.encodings[&#39;input_ids&#39;]) . from transformers import AutoTokenizer MODEL_NAME = &#39;bert-base-german-cased&#39; tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) . https://huggingface.co/bert-base-german-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpu0r451i2 storing https://huggingface.co/bert-base-german-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/2529d64cc99a539f2103ad09cea0d6459e181d8dc168fe06b32d25ddc68e6d3b.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f creating metadata file for /root/.cache/huggingface/transformers/2529d64cc99a539f2103ad09cea0d6459e181d8dc168fe06b32d25ddc68e6d3b.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f . . https://huggingface.co/bert-base-german-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp17luf07x storing https://huggingface.co/bert-base-german-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/98877e98ee76b3977d326fe4f54bc29f10b486c317a70b6445ac19a0603b00f0.1f2afedb22f9784795ae3a26fe20713637c93f50e2c99101d952ea6476087e5e creating metadata file for /root/.cache/huggingface/transformers/98877e98ee76b3977d326fe4f54bc29f10b486c317a70b6445ac19a0603b00f0.1f2afedb22f9784795ae3a26fe20713637c93f50e2c99101d952ea6476087e5e loading configuration file https://huggingface.co/bert-base-german-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/98877e98ee76b3977d326fe4f54bc29f10b486c317a70b6445ac19a0603b00f0.1f2afedb22f9784795ae3a26fe20713637c93f50e2c99101d952ea6476087e5e Model config BertConfig { &#34;architectures&#34;: [ &#34;BertForMaskedLM&#34; ], &#34;attention_probs_dropout_prob&#34;: 0.1, &#34;gradient_checkpointing&#34;: false, &#34;hidden_act&#34;: &#34;gelu&#34;, &#34;hidden_dropout_prob&#34;: 0.1, &#34;hidden_size&#34;: 768, &#34;initializer_range&#34;: 0.02, &#34;intermediate_size&#34;: 3072, &#34;layer_norm_eps&#34;: 1e-12, &#34;max_position_embeddings&#34;: 512, &#34;model_type&#34;: &#34;bert&#34;, &#34;num_attention_heads&#34;: 12, &#34;num_hidden_layers&#34;: 12, &#34;pad_token_id&#34;: 0, &#34;position_embedding_type&#34;: &#34;absolute&#34;, &#34;transformers_version&#34;: &#34;4.8.2&#34;, &#34;type_vocab_size&#34;: 2, &#34;use_cache&#34;: true, &#34;vocab_size&#34;: 30000 } . . https://huggingface.co/bert-base-german-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpgtu9z249 storing https://huggingface.co/bert-base-german-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/0c57cb5172c1ac6c957d00597dc43c1b8b2a2cb44729a590fd0112612221f746.9a4f439638381be22bb9f116542bdaa5e1d8bb7a09a5f8ef32d9662deaf655a1 creating metadata file for /root/.cache/huggingface/transformers/0c57cb5172c1ac6c957d00597dc43c1b8b2a2cb44729a590fd0112612221f746.9a4f439638381be22bb9f116542bdaa5e1d8bb7a09a5f8ef32d9662deaf655a1 . . https://huggingface.co/bert-base-german-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp7ssc0k_c storing https://huggingface.co/bert-base-german-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/a60c7a72be0cad1606096bd88aa22980c826a10b2482a850cfd50db5ceb3f01f.a1d3fa1580dc5318a8ad0477d679498575453bbe1ef5751aaca7fec558055f77 creating metadata file for /root/.cache/huggingface/transformers/a60c7a72be0cad1606096bd88aa22980c826a10b2482a850cfd50db5ceb3f01f.a1d3fa1580dc5318a8ad0477d679498575453bbe1ef5751aaca7fec558055f77 . . loading file https://huggingface.co/bert-base-german-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/0c57cb5172c1ac6c957d00597dc43c1b8b2a2cb44729a590fd0112612221f746.9a4f439638381be22bb9f116542bdaa5e1d8bb7a09a5f8ef32d9662deaf655a1 loading file https://huggingface.co/bert-base-german-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/a60c7a72be0cad1606096bd88aa22980c826a10b2482a850cfd50db5ceb3f01f.a1d3fa1580dc5318a8ad0477d679498575453bbe1ef5751aaca7fec558055f77 loading file https://huggingface.co/bert-base-german-cased/resolve/main/added_tokens.json from cache at None loading file https://huggingface.co/bert-base-german-cased/resolve/main/special_tokens_map.json from cache at None loading file https://huggingface.co/bert-base-german-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/2529d64cc99a539f2103ad09cea0d6459e181d8dc168fe06b32d25ddc68e6d3b.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f . def predict_text(text, train_tags=train_tags, trainer=trainer, tokenizer=tokenizer): train_encodings = tokenizer([text], is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True) train_encodings.pop(&quot;offset_mapping&quot;) # we don&#39;t want to pass this to the model train_dataset = COURTS_Dataset_inf(train_encodings) result = trainer.predict(train_dataset) token_ids = tokenizer.encode(text[0], add_special_tokens=False) word_tag_dic = [] preds = np.argmax(result.predictions, axis=2) x=1 for word in text[0].split(&#39; &#39;): word_tag_dic.append([word, id2tag[preds[0][x]]]) x=x+1 return word_tag_dic . import numpy as np . word_tag_dic=predict_text([&#39; &#39;.join(train_texts[3])]) . ***** Running Prediction ***** Num examples = 1 Batch size = 64 . . [1/1 : &lt; :] word_tag_dic . [[&#39;Die&#39;, &#39;B-RS&#39;], [&#39;beabsichtigte&#39;, &#39;B-RS&#39;], [&#39;Klage&#39;, &#39;B-RS&#39;], [&#39;auf&#39;, &#39;B-RS&#39;], [&#39;EntschÃ¤digung&#39;, &#39;B-RS&#39;], [&#39;wegen&#39;, &#39;B-RS&#39;], [&#39;der&#39;, &#39;B-RS&#39;], [&#39;Dauer&#39;, &#39;B-RS&#39;], [&#39;des&#39;, &#39;B-RS&#39;], [&#39;Verfahrens&#39;, &#39;I-INN&#39;], [&#39;vor&#39;, &#39;B-LIT&#39;], [&#39;dem&#39;, &#39;B-LIT&#39;], [&#39;Bundesarbeitsgericht&#39;, &#39;B-LIT&#39;], [&#39;zum&#39;, &#39;B-LIT&#39;], [&#39;Aktenzeichen&#39;, &#39;B-LIT&#39;], [&#39;-&#39;, &#39;B-LIT&#39;], [&#39;8&#39;, &#39;B-LIT&#39;], [&#39;AZR&#39;, &#39;B-LIT&#39;], [&#39;418/15&#39;, &#39;B-LIT&#39;], [&#39;-&#39;, &#39;B-LIT&#39;], [&#39;bietet&#39;, &#39;B-LIT&#39;], [&#39;keine&#39;, &#39;B-LIT&#39;], [&#39;hinreichende&#39;, &#39;B-LIT&#39;], [&#39;Aussicht&#39;, &#39;B-LIT&#39;], [&#39;auf&#39;, &#39;B-LIT&#39;], [&#39;Erfolg&#39;, &#39;B-RS&#39;], [&#39;,&#39;, &#39;B-RS&#39;], [&#39;Â§&#39;, &#39;B-RS&#39;], [&#39;114&#39;, &#39;B-RS&#39;], [&#39;Abs.&#39;, &#39;B-RS&#39;], [&#39;1&#39;, &#39;B-RS&#39;], [&#39;Satz&#39;, &#39;B-RS&#39;], [&#39;1&#39;, &#39;B-LDS&#39;], [&#39;ZPO&#39;, &#39;B-VT&#39;], [&#39;.&#39;, &#39;B-VT&#39;]] . from spacy.vocab import Vocab from spacy.tokens.doc import Doc from spacy.tokens import Span . def predict_and_visulize(text, train_tags=train_tags, trainer=trainer, tokenizer=tokenizer): word_tag_dic=predict_text([&#39; &#39;.join(text)]) x=0 span_list = [] vocab = Vocab(strings=text) doc = Doc(vocab, words=text) for token in word_tag_dic: # print(x) span_list.append(Span(doc, x, x+1, token[1])) x=x+1 doc.set_ents(span_list) return doc . from spacy import displacy doc = predict_and_visulize(train_texts[6]) displacy.render(doc , jupyter=True, style=&#39;ent&#39;) . ***** Running Prediction ***** Num examples = 1 Batch size = 64 . . [1/1 00:53] Dementsprechend B-RS hat B-RS der B-RS Bundesgerichtshof I-ST mit B-UN Beschluss B-RS vom I-INN 24. B-LIT August B-LIT 2017 B-LIT ( B-LIT - B-LIT III B-LIT ZA B-LIT 15/17 B-LIT - B-LIT ) B-LIT das B-LIT bei B-LIT ihm B-LIT von B-LIT der B-LIT Antragstellerin B-RS anhÃ¤ngig B-RS gemachte B-RS â€ž B-RS ProzesskostenhilfeprÃ¼fungsverfahren B-RS â€œ B-RS an B-RS das B-RS Bundesarbeitsgericht B-RS abgegeben B-RS . B-RS",
            "url": "https://jonathanpro.github.io/myaiblog/jupyter/nlp/2021/07/10/huggingface_ner_pytorch_inference.html",
            "relUrl": "/jupyter/nlp/2021/07/10/huggingface_ner_pytorch_inference.html",
            "date": " â€¢ Jul 10, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "NLP -  Named Entity Recognition - Training",
            "content": ". from google.colab import drive drive.mount(&#39;/content/gdrive&#39;) . Mounted at /content/gdrive . !curl https://github.com/elenanereiss/Legal-Entity-Recognition/raw/master/data/dataset_courts.zip -L -o /content/raw.zip . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 168 100 168 0 0 282 0 --:--:-- --:--:-- --:--:-- 282 100 4289k 100 4289k 0 0 3338k 0 0:00:01 0:00:01 --:--:-- 19.2M . !unzip /content/raw.zip -d /content/raw/ . Archive: /content/raw.zip inflating: /content/raw/bfh.conll inflating: /content/raw/bgh.conll inflating: /content/raw/bpatg.conll inflating: /content/raw/bsg.conll inflating: /content/raw/bverfg.conll inflating: /content/raw/bverwg.conll inflating: /content/raw/bag.conll . !pip install transformers . Collecting transformers Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.5MB 7.0MB/s Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12) Collecting sacremoses Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 901kB 34.3MB/s Collecting tokenizers&lt;0.11,&gt;=0.10.1 Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 38.8MB/s Requirement already satisfied: importlib-metadata; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0) Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1) Requirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5) Collecting huggingface-hub==0.0.12 Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0) Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9) Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers) (7.1.2) Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers) (1.0.1) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers) (1.15.0) Requirement already satisfied: typing-extensions&gt;=3.6.4; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;transformers) (3.7.4.3) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;transformers) (3.4.1) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (2021.5.30) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (1.24.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (2.10) Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;transformers) (2.4.7) Installing collected packages: sacremoses, tokenizers, huggingface-hub, transformers Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2 . !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl . Collecting cloud-tpu-client==0.10 Downloading https://files.pythonhosted.org/packages/56/9f/7b1958c2886db06feb5de5b2c191096f9e619914b6c31fdf93999fdbbd8b/cloud_tpu_client-0.10-py3-none-any.whl Collecting torch-xla==1.9 Downloading https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl (149.9MB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149.9MB 77kB/s Collecting google-api-python-client==1.8.0 Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61kB 3.1MB/s Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (4.1.3) Requirement already satisfied: google-auth&gt;=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (1.31.0) Requirement already satisfied: httplib2&lt;1dev,&gt;=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (0.17.4) Requirement already satisfied: google-auth-httplib2&gt;=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (0.0.4) Requirement already satisfied: uritemplate&lt;4dev,&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (3.0.1) Requirement already satisfied: google-api-core&lt;2dev,&gt;=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (1.26.3) Requirement already satisfied: six&lt;2dev,&gt;=1.6.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (1.15.0) Requirement already satisfied: rsa&gt;=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client-&gt;cloud-tpu-client==0.10) (4.7.2) Requirement already satisfied: pyasn1&gt;=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client-&gt;cloud-tpu-client==0.10) (0.4.8) Requirement already satisfied: pyasn1-modules&gt;=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client-&gt;cloud-tpu-client==0.10) (0.2.8) Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth&gt;=1.4.1-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (4.2.2) Requirement already satisfied: setuptools&gt;=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth&gt;=1.4.1-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (57.0.0) Requirement already satisfied: requests&lt;3.0.0dev,&gt;=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (2.23.0) Requirement already satisfied: packaging&gt;=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (20.9) Requirement already satisfied: protobuf&gt;=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (3.12.4) Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (2018.9) Requirement already satisfied: googleapis-common-protos&lt;2.0dev,&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (1.53.0) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (2021.5.30) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (1.24.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (2.10) Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging&gt;=14.3-&gt;google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (2.4.7) ERROR: earthengine-api 0.1.269 has requirement google-api-python-client&lt;2,&gt;=1.12.1, but you&#39;ll have google-api-python-client 1.8.0 which is incompatible. Installing collected packages: google-api-python-client, cloud-tpu-client, torch-xla Found existing installation: google-api-python-client 1.12.8 Uninstalling google-api-python-client-1.12.8: Successfully uninstalled google-api-python-client-1.12.8 Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0 torch-xla-1.9 . from pathlib import Path import re def read_wnut(file_path): file_path = Path(file_path) raw_text = file_path.read_text().strip() raw_docs = re.split(r&#39; n t? n&#39;, raw_text) token_docs = [] tag_docs = [] for doc in raw_docs: tokens = [] tags = [] for line in doc.split(&#39; n&#39;): # print(line) token, tag = line.split() tokens.append(token) tags.append(tag) token_docs.append(tokens) tag_docs.append(tags) return token_docs, tag_docs train_texts, train_tags = read_wnut(&#39;/content/raw/bag.conll&#39;) val_texts, val_tags = read_wnut(&#39;/content/raw/bgh.conll&#39;) . tags = train_tags + val_tags . . unique_tags = set(tag for doc in tags for tag in doc) tag2id = {tag: id for id, tag in enumerate(unique_tags)} id2tag = {id: tag for tag, id in tag2id.items()} . from transformers import AutoTokenizer MODEL_NAME = &#39;bert-base-german-cased&#39; tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True) val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True) . . import numpy as np def encode_tags(tags, encodings): labels = [[tag2id[tag] for tag in doc] for doc in tags] encoded_labels = [] for doc_labels, doc_offset in zip(labels, encodings.offset_mapping): # create an empty array of -100 doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100 arr_offset = np.array(doc_offset) # set labels whose first offset position is 0 and the second is not 0 doc_enc_labels[(arr_offset[:,0] == 0) &amp; (arr_offset[:,1] != 0)] = doc_labels encoded_labels.append(doc_enc_labels.tolist()) return encoded_labels train_labels = encode_tags(train_tags, train_encodings) val_labels = encode_tags(val_tags, val_encodings) . import torch class COURTS_Dataset_train(torch.utils.data.Dataset): def __init__(self, encodings, labels): self.encodings = encodings self.labels = labels def __getitem__(self, idx): item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} item[&#39;labels&#39;] = torch.tensor(self.labels[idx]) return item def __len__(self): return len(self.labels) train_encodings.pop(&quot;offset_mapping&quot;) # we don&#39;t want to pass this to the model val_encodings.pop(&quot;offset_mapping&quot;) train_dataset = COURTS_Dataset_train(train_encodings, train_labels) val_dataset = COURTS_Dataset_train(val_encodings, val_labels) . from transformers import AutoModelForTokenClassification model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(unique_tags)) . . Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForTokenClassification: [&#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.predictions.bias&#39;, &#39;cls.seq_relationship.weight&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.decoder.weight&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;] - This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: [&#39;classifier.bias&#39;, &#39;classifier.weight&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . Set device to TPU . import torch_xla import torch_xla.core.xla_model as xm device = xm.xla_device() . WARNING:root:Waiting for TPU to be start up with version pytorch-1.9... WARNING:root:Waiting for TPU to be start up with version pytorch-1.9... WARNING:root:TPU has started up successfully with version pytorch-1.9 . from transformers import Trainer, TrainingArguments training_args = TrainingArguments( output_dir=&#39;./results&#39;, # output directory num_train_epochs=30, # total number of training epochs per_device_train_batch_size=16, # batch size per device during training per_device_eval_batch_size=64, # batch size for evaluation warmup_steps=500, # number of warmup steps for learning rate scheduler weight_decay=0.01, # strength of weight decay logging_dir=&#39;./logs&#39;, # directory for storing logs logging_steps=5000, ) # Map model to TPU model = model.to(device) trainer = Trainer( model=model, # the instantiated ðŸ¤— Transformers model to be trained args=training_args, # training arguments, defined above train_dataset=train_dataset, # training dataset eval_dataset=val_dataset # evaluation dataset ) . trainer.train() . ***** Running training ***** Num examples = 12791 Num Epochs = 30 Instantaneous batch size per device = 16 Total train batch size (w. parallel, distributed &amp; accumulation) = 16 Gradient Accumulation steps = 1 Total optimization steps = 24000 . . [21642/24000 2:26:05 &lt; 15:55, 2.47 it/s, Epoch 27.05/30] Step Training Loss . 5000 | 0.058200 | . 10000 | 0.003400 | . 15000 | 0.001700 | . 20000 | 0.000800 | . &lt;/div&gt; &lt;/div&gt; Saving model checkpoint to ./results/checkpoint-500 Configuration saved in ./results/checkpoint-500/config.json Model weights saved in ./results/checkpoint-500/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-1000 Configuration saved in ./results/checkpoint-1000/config.json Model weights saved in ./results/checkpoint-1000/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-1500 Configuration saved in ./results/checkpoint-1500/config.json Model weights saved in ./results/checkpoint-1500/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-2000 Configuration saved in ./results/checkpoint-2000/config.json Model weights saved in ./results/checkpoint-2000/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-2500 Configuration saved in ./results/checkpoint-2500/config.json Model weights saved in ./results/checkpoint-2500/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-3000 Configuration saved in ./results/checkpoint-3000/config.json Model weights saved in ./results/checkpoint-3000/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-3500 Configuration saved in ./results/checkpoint-3500/config.json Model weights saved in ./results/checkpoint-3500/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-4000 Configuration saved in ./results/checkpoint-4000/config.json Model weights saved in ./results/checkpoint-4000/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-4500 Configuration saved in ./results/checkpoint-4500/config.json Model weights saved in ./results/checkpoint-4500/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-5000 Configuration saved in ./results/checkpoint-5000/config.json Model weights saved in ./results/checkpoint-5000/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-5500 Configuration saved in ./results/checkpoint-5500/config.json Model weights saved in ./results/checkpoint-5500/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-6000 Configuration saved in ./results/checkpoint-6000/config.json Model weights saved in ./results/checkpoint-6000/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-6500 Configuration saved in ./results/checkpoint-6500/config.json Model weights saved in ./results/checkpoint-6500/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-7000 Configuration saved in ./results/checkpoint-7000/config.json Model weights saved in ./results/checkpoint-7000/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-7500 Configuration saved in ./results/checkpoint-7500/config.json Model weights saved in ./results/checkpoint-7500/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-8000 Configuration saved in ./results/checkpoint-8000/config.json Model weights saved in ./results/checkpoint-8000/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-8500 Configuration saved in ./results/checkpoint-8500/config.json Model weights saved in ./results/checkpoint-8500/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-9000 Configuration saved in ./results/checkpoint-9000/config.json Model weights saved in ./results/checkpoint-9000/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-9500 Configuration saved in ./results/checkpoint-9500/config.json Model weights saved in ./results/checkpoint-9500/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-10000 Configuration saved in ./results/checkpoint-10000/config.json Model weights saved in ./results/checkpoint-10000/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-10500 Configuration saved in ./results/checkpoint-10500/config.json Model weights saved in ./results/checkpoint-10500/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-11000 Configuration saved in ./results/checkpoint-11000/config.json Model weights saved in ./results/checkpoint-11000/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-11500 Configuration saved in ./results/checkpoint-11500/config.json Model weights saved in ./results/checkpoint-11500/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-12000 Configuration saved in ./results/checkpoint-12000/config.json Model weights saved in ./results/checkpoint-12000/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-12500 Configuration saved in ./results/checkpoint-12500/config.json Model weights saved in ./results/checkpoint-12500/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-13000 Configuration saved in ./results/checkpoint-13000/config.json Model weights saved in ./results/checkpoint-13000/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-13500 Configuration saved in ./results/checkpoint-13500/config.json Model weights saved in ./results/checkpoint-13500/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-14000 Configuration saved in ./results/checkpoint-14000/config.json Model weights saved in ./results/checkpoint-14000/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-14500 Configuration saved in ./results/checkpoint-14500/config.json Model weights saved in ./results/checkpoint-14500/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-15000 Configuration saved in ./results/checkpoint-15000/config.json Model weights saved in ./results/checkpoint-15000/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-15500 Configuration saved in ./results/checkpoint-15500/config.json Model weights saved in ./results/checkpoint-15500/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-16000 Configuration saved in ./results/checkpoint-16000/config.json Model weights saved in ./results/checkpoint-16000/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-16500 Configuration saved in ./results/checkpoint-16500/config.json Model weights saved in ./results/checkpoint-16500/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-17000 Configuration saved in ./results/checkpoint-17000/config.json Model weights saved in ./results/checkpoint-17000/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-17500 Configuration saved in ./results/checkpoint-17500/config.json Model weights saved in ./results/checkpoint-17500/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-18000 Configuration saved in ./results/checkpoint-18000/config.json Model weights saved in ./results/checkpoint-18000/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-18500 Configuration saved in ./results/checkpoint-18500/config.json Model weights saved in ./results/checkpoint-18500/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-19000 Configuration saved in ./results/checkpoint-19000/config.json Model weights saved in ./results/checkpoint-19000/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-19500 Configuration saved in ./results/checkpoint-19500/config.json Model weights saved in ./results/checkpoint-19500/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-20000 Configuration saved in ./results/checkpoint-20000/config.json Model weights saved in ./results/checkpoint-20000/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-20500 Configuration saved in ./results/checkpoint-20500/config.json Model weights saved in ./results/checkpoint-20500/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-21000 Configuration saved in ./results/checkpoint-21000/config.json Model weights saved in ./results/checkpoint-21000/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-21500 Configuration saved in ./results/checkpoint-21500/config.json Model weights saved in ./results/checkpoint-21500/pytorch_model.bin . . [24000/24000 2:41:52, Epoch 30/30] Step Training Loss . 5000 | 0.058200 | . 10000 | 0.003400 | . 15000 | 0.001700 | . 20000 | 0.000800 | . &lt;/div&gt; &lt;/div&gt; Saving model checkpoint to ./results/checkpoint-22000 Configuration saved in ./results/checkpoint-22000/config.json Model weights saved in ./results/checkpoint-22000/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-22500 Configuration saved in ./results/checkpoint-22500/config.json Model weights saved in ./results/checkpoint-22500/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-23000 Configuration saved in ./results/checkpoint-23000/config.json Model weights saved in ./results/checkpoint-23000/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-23500 Configuration saved in ./results/checkpoint-23500/config.json Model weights saved in ./results/checkpoint-23500/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-24000 Configuration saved in ./results/checkpoint-24000/config.json Model weights saved in ./results/checkpoint-24000/pytorch_model.bin Training completed. Do not forget to share your model on huggingface.co/models =) . TrainOutput(global_step=24000, training_loss=0.013400553142031034, metrics={&#39;train_runtime&#39;: 9714.0783, &#39;train_samples_per_second&#39;: 39.502, &#39;train_steps_per_second&#39;: 2.471, &#39;total_flos&#39;: 8.345190008162549e+16, &#39;train_loss&#39;: 0.013400553142031034, &#39;epoch&#39;: 30.0}) . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; trainer.save_model(&quot;/content/gdrive/MyDrive/NER_01_030_2021_07_08.bin&quot;) . Saving model checkpoint to /content/gdrive/MyDrive/NER_01_030_2021_07_08.bin Configuration saved in /content/gdrive/MyDrive/NER_01_030_2021_07_08.bin/config.json Model weights saved in /content/gdrive/MyDrive/NER_01_030_2021_07_08.bin/pytorch_model.bin . class COURTS_Dataset_inf(torch.utils.data.Dataset): def __init__(self, encodings): self.encodings = encodings def __getitem__(self, idx): item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} return item def __len__(self): return len(self.encodings[&#39;input_ids&#39;]) . sentence = &#39; &#39;.join(train_texts[0]) train_texts[0] . [&#39;Prozesskostenhilfe&#39;, &#39;-&#39;, &#39;EntschÃ¤digung&#39;, &#39;fÃ¼r&#39;, &#39;Ã¼berlange&#39;, &#39;Verfahrensdauer&#39;, &#39;-&#39;, &#39;Revisionsverfahren&#39;] . def predict_text(text, train_tags=train_tags, trainer=trainer, tokenizer=tokenizer): train_encodings = tokenizer([text], is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True) train_encodings.pop(&quot;offset_mapping&quot;) # we don&#39;t want to pass this to the model train_dataset = COURTS_Dataset_inf(train_encodings) result = trainer.predict(train_dataset) token_ids = tokenizer.encode(text[0], add_special_tokens=False) word_tag_dic = [] preds = np.argmax(result.predictions, axis=2) x=1 for word in text[0].split(&#39; &#39;): word_tag_dic.append([word, id2tag[preds[0][x]]]) x=x+1 return word_tag_dic . word_tag_dic=predict_text([&#39; &#39;.join(train_texts[3])]) . ***** Running Prediction ***** Num examples = 1 Batch size = 64 . . [1/1 : &lt; :] [&#39; &#39;.join(train_texts[3])] . [&#39;Die beabsichtigte Klage auf EntschÃ¤digung wegen der Dauer des Verfahrens vor dem Bundesarbeitsgericht zum Aktenzeichen - 8 AZR 418/15 - bietet keine hinreichende Aussicht auf Erfolg , Â§ 114 Abs. 1 Satz 1 ZPO .&#39;] . train_texts[3] . [&#39;Die&#39;, &#39;beabsichtigte&#39;, &#39;Klage&#39;, &#39;auf&#39;, &#39;EntschÃ¤digung&#39;, &#39;wegen&#39;, &#39;der&#39;, &#39;Dauer&#39;, &#39;des&#39;, &#39;Verfahrens&#39;, &#39;vor&#39;, &#39;dem&#39;, &#39;Bundesarbeitsgericht&#39;, &#39;zum&#39;, &#39;Aktenzeichen&#39;, &#39;-&#39;, &#39;8&#39;, &#39;AZR&#39;, &#39;418/15&#39;, &#39;-&#39;, &#39;bietet&#39;, &#39;keine&#39;, &#39;hinreichende&#39;, &#39;Aussicht&#39;, &#39;auf&#39;, &#39;Erfolg&#39;, &#39;,&#39;, &#39;Â§&#39;, &#39;114&#39;, &#39;Abs.&#39;, &#39;1&#39;, &#39;Satz&#39;, &#39;1&#39;, &#39;ZPO&#39;, &#39;.&#39;] . train_tags[3] . [&#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-RS&#39;, &#39;I-RS&#39;, &#39;I-RS&#39;, &#39;I-RS&#39;, &#39;I-RS&#39;, &#39;I-RS&#39;, &#39;I-RS&#39;, &#39;I-RS&#39;, &#39;I-RS&#39;, &#39;I-RS&#39;, &#39;I-RS&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-GS&#39;, &#39;I-GS&#39;, &#39;I-GS&#39;, &#39;I-GS&#39;, &#39;I-GS&#39;, &#39;I-GS&#39;, &#39;I-GS&#39;, &#39;O&#39;] . &lt;/div&gt; .",
            "url": "https://jonathanpro.github.io/myaiblog/jupyter/nlp/tpu/2021/07/09/huggingface_ner_pytorch_training.html",
            "relUrl": "/jupyter/nlp/tpu/2021/07/09/huggingface_ner_pytorch_training.html",
            "date": " â€¢ Jul 9, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "NLP -  Multi Class Classification - Training",
            "content": ". Usecase: Classify titles of news headline to specific categories . Train model . Example: &quot;Kim Kardashian Flashes Cleavage As She Arrives...&quot; is mapped to &quot;Entertainment&quot; . !pip install transformers . Collecting transformers Downloading https://files.pythonhosted.org/packages/b5/d5/c6c23ad75491467a9a84e526ef2364e523d45e2b0fae28a7cbe8689e7e84/transformers-4.8.1-py3-none-any.whl (2.5MB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.5MB 5.0MB/s Requirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9) Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1) Collecting huggingface-hub==0.0.12 Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0) Collecting tokenizers&lt;0.11,&gt;=0.10.1 Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 30.3MB/s Requirement already satisfied: importlib-metadata; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0) Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12) Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20) Collecting sacremoses Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 901kB 36.6MB/s Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;transformers) (2.4.7) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12-&gt;transformers) (3.7.4.3) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (2021.5.30) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (1.24.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (2.10) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;transformers) (3.4.1) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers) (1.15.0) Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers) (1.0.1) Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers) (7.1.2) Installing collected packages: huggingface-hub, tokenizers, sacremoses, transformers Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.1 . Optional for using TPU. Make sure to select TPU from Edit &gt; Notebook settings &gt; Hardware accelerator . !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl . Collecting cloud-tpu-client==0.10 Downloading https://files.pythonhosted.org/packages/56/9f/7b1958c2886db06feb5de5b2c191096f9e619914b6c31fdf93999fdbbd8b/cloud_tpu_client-0.10-py3-none-any.whl Collecting torch-xla==1.9 Downloading https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl (149.9MB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149.9MB 76kB/s Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (4.1.3) Collecting google-api-python-client==1.8.0 Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61kB 2.9MB/s Requirement already satisfied: rsa&gt;=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client-&gt;cloud-tpu-client==0.10) (4.7.2) Requirement already satisfied: six&gt;=1.6.1 in /usr/local/lib/python3.7/dist-packages (from oauth2client-&gt;cloud-tpu-client==0.10) (1.15.0) Requirement already satisfied: httplib2&gt;=0.9.1 in /usr/local/lib/python3.7/dist-packages (from oauth2client-&gt;cloud-tpu-client==0.10) (0.17.4) Requirement already satisfied: pyasn1&gt;=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client-&gt;cloud-tpu-client==0.10) (0.4.8) Requirement already satisfied: pyasn1-modules&gt;=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client-&gt;cloud-tpu-client==0.10) (0.2.8) Requirement already satisfied: google-auth&gt;=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (1.31.0) Requirement already satisfied: google-api-core&lt;2dev,&gt;=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (1.26.3) Requirement already satisfied: google-auth-httplib2&gt;=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (0.0.4) Requirement already satisfied: uritemplate&lt;4dev,&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (3.0.1) Requirement already satisfied: setuptools&gt;=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth&gt;=1.4.1-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (57.0.0) Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth&gt;=1.4.1-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (4.2.2) Requirement already satisfied: protobuf&gt;=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (3.12.4) Requirement already satisfied: requests&lt;3.0.0dev,&gt;=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (2.23.0) Requirement already satisfied: packaging&gt;=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (20.9) Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (2018.9) Requirement already satisfied: googleapis-common-protos&lt;2.0dev,&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (1.53.0) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (2021.5.30) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (3.0.4) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (1.24.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (2.10) Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging&gt;=14.3-&gt;google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (2.4.7) ERROR: earthengine-api 0.1.269 has requirement google-api-python-client&lt;2,&gt;=1.12.1, but you&#39;ll have google-api-python-client 1.8.0 which is incompatible. Installing collected packages: google-api-python-client, cloud-tpu-client, torch-xla Found existing installation: google-api-python-client 1.12.8 Uninstalling google-api-python-client-1.12.8: Successfully uninstalled google-api-python-client-1.12.8 Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0 torch-xla-1.9 . import pandas as pd import torch import transformers from torch.utils.data import Dataset, DataLoader from transformers import DistilBertModel, DistilBertTokenizer . from torch import cuda device = &#39;cuda&#39; if cuda.is_available() else &#39;cpu&#39; . Download public data to use in the notebook . ! wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip . --2021-06-29 07:22:42-- https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252 Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 29224203 (28M) [application/x-httpd-php] Saving to: â€˜NewsAggregatorDataset.zipâ€™ NewsAggregatorDatas 100%[===================&gt;] 27.87M 32.6MB/s in 0.9s 2021-06-29 07:22:43 (32.6 MB/s) - â€˜NewsAggregatorDataset.zipâ€™ saved [29224203/29224203] . !unzip NewsAggregatorDataset.zip . Archive: NewsAggregatorDataset.zip inflating: 2pageSessions.csv creating: __MACOSX/ inflating: __MACOSX/._2pageSessions.csv inflating: newsCorpora.csv inflating: __MACOSX/._newsCorpora.csv inflating: readme.txt inflating: __MACOSX/._readme.txt . !ls . 2pageSessions.csv NewsAggregatorDataset.zip readme.txt __MACOSX newsCorpora.csv sample_data . Import the csv into pandas dataframe and add the headers . df = pd.read_csv(&#39;newsCorpora.csv&#39;, sep=&#39; t&#39;, names=[&#39;ID&#39;,&#39;TITLE&#39;, &#39;URL&#39;, &#39;PUBLISHER&#39;, &#39;CATEGORY&#39;, &#39;STORY&#39;, &#39;HOSTNAME&#39;, &#39;TIMESTAMP&#39;]) . Choose a sample based on our available processing power . . df = df[[&#39;TITLE&#39;,&#39;CATEGORY&#39;]] # Converting the codes to appropriate categories using a dictionary my_dict = { &#39;e&#39;:&#39;Entertainment&#39;, &#39;b&#39;:&#39;Business&#39;, &#39;t&#39;:&#39;Science&#39;, &#39;m&#39;:&#39;Health&#39; } def update_cat(x): return my_dict[x] df[&#39;CATEGORY&#39;] = df[&#39;CATEGORY&#39;].apply(lambda x: update_cat(x)) encode_dict = {} def encode_cat(x): if x not in encode_dict.keys(): encode_dict[x]=len(encode_dict) return encode_dict[x] df[&#39;ENCODE_CAT&#39;] = df[&#39;CATEGORY&#39;].apply(lambda x: encode_cat(x)) . Our actuall data that is used . df.head() . TITLE CATEGORY ENCODE_CAT . 0 Fed official says weak data caused by weather,... | Business | 0 | . 1 Fed&#39;s Charles Plosser sees high bar for change... | Business | 0 | . 2 US open: Stocks fall after Fed official hints ... | Business | 0 | . 3 Fed risks falling &#39;behind the curve&#39;, Charles ... | Business | 0 | . 4 Fed&#39;s Plosser: Nasty Weather Has Curbed Job Gr... | Business | 0 | . List of a categories . labels = list(df[&#39;CATEGORY&#39;].unique()) . df_mapping=df[[&#39;CATEGORY&#39;, &#39;ENCODE_CAT&#39;]].drop_duplicates() df_mapping = df_mapping.reset_index() id2label = pd.Series(df_mapping.CATEGORY,index=df_mapping.ENCODE_CAT).to_dict() . MAX_LEN = 512 TRAIN_BATCH_SIZE = 4 VALID_BATCH_SIZE = 2 EPOCHS = 1 LEARNING_RATE = 1e-05 tokenizer = DistilBertTokenizer.from_pretrained(&#39;distilbert-base-cased&#39;, id2label=id2label) . . class newsCorpora_data(Dataset): def __init__(self, dataframe, tokenizer, max_len): self.len = len(dataframe) self.data = dataframe self.tokenizer = tokenizer self.max_len = max_len def __getitem__(self, index): title = str(self.data.TITLE[index]) title = &quot; &quot;.join(title.split()) inputs = self.tokenizer.encode_plus( title, None, add_special_tokens=True, max_length=self.max_len, pad_to_max_length=True, return_token_type_ids=True, truncation=True ) ids = inputs[&#39;input_ids&#39;] mask = inputs[&#39;attention_mask&#39;] inputs[&#39;label&#39;] = torch.tensor(self.data.ENCODE_CAT[index], dtype=torch.long) del inputs[&#39;token_type_ids&#39;] return inputs &quot;&quot;&quot; return { &#39;ids&#39;: torch.tensor(ids, dtype=torch.long), &#39;mask&#39;: torch.tensor(mask, dtype=torch.long), &#39;label&#39;: torch.tensor(self.data.ENCODE_CAT[index], dtype=torch.long) } &quot;&quot;&quot; def __len__(self): return self.len . train_size = 0.8 train_dataset = df.sample(frac=train_size,random_state=200) test_dataset = df.drop(train_dataset.index).reset_index(drop=True) train_dataset = train_dataset.reset_index(drop=True) training_set = newsCorpora_data(train_dataset, tokenizer, MAX_LEN) testing_set = newsCorpora_data(test_dataset, tokenizer, MAX_LEN) . Set device to TPU . import torch_xla import torch_xla.core.xla_model as xm device = xm.xla_device() . WARNING:root:Waiting for TPU to be start up with version pytorch-1.9... WARNING:root:Waiting for TPU to be start up with version pytorch-1.9... WARNING:root:TPU has started up successfully with version pytorch-1.9 . from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments training_args = TrainingArguments( output_dir=&#39;./results&#39;, # output directory num_train_epochs=3, # total number of training epochs per_device_train_batch_size=1*16, # batch size per device during training per_device_eval_batch_size=1*64, # batch size for evaluation warmup_steps=500, # number of warmup steps for learning rate scheduler weight_decay=0.01, # strength of weight decay logging_dir=&#39;./logs&#39;, # directory for storing logs logging_steps=5000, save_strategy=&#39;steps&#39;, save_steps=5000 ) model = AutoModelForSequenceClassification.from_pretrained(&quot;distilbert-base-cased&quot;, num_labels=len(labels), id2label=id2label) # Map model to TPU model = model.to(device) trainer = Trainer( model=model, # the instantiated ðŸ¤— Transformers model to be trained args=training_args, # training arguments, defined above train_dataset=training_set, # training dataset eval_dataset=testing_set # evaluation dataset ) . . Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: [&#39;vocab_transform.weight&#39;, &#39;vocab_projector.bias&#39;, &#39;vocab_layer_norm.weight&#39;, &#39;vocab_projector.weight&#39;, &#39;vocab_layer_norm.bias&#39;, &#39;vocab_transform.bias&#39;] - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: [&#39;pre_classifier.bias&#39;, &#39;classifier.bias&#39;, &#39;classifier.weight&#39;, &#39;pre_classifier.weight&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . trainer.train() . ***** Running training ***** Num examples = 337935 Num Epochs = 3 Instantaneous batch size per device = 16 Total train batch size (w. parallel, distributed &amp; accumulation) = 16 Gradient Accumulation steps = 1 Total optimization steps = 63363 /usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding=&#39;longest&#39;` to pad to the longest sequence in the batch, or use `padding=&#39;max_length&#39;` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert). FutureWarning, . . [63363/63363 4:42:44, Epoch 3/3] Step Training Loss . 5000 | 0.347500 | . 10000 | 0.224900 | . 15000 | 0.203000 | . 20000 | 0.190500 | . 25000 | 0.144800 | . 30000 | 0.135700 | . 35000 | 0.131100 | . 40000 | 0.128900 | . 45000 | 0.104000 | . 50000 | 0.085600 | . 55000 | 0.082500 | . 60000 | 0.083400 | . &lt;/div&gt; &lt;/div&gt; Saving model checkpoint to ./results/checkpoint-5000 Configuration saved in ./results/checkpoint-5000/config.json Model weights saved in ./results/checkpoint-5000/pytorch_model.bin /usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding=&#39;longest&#39;` to pad to the longest sequence in the batch, or use `padding=&#39;max_length&#39;` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert). FutureWarning, Saving model checkpoint to ./results/checkpoint-10000 Configuration saved in ./results/checkpoint-10000/config.json Model weights saved in ./results/checkpoint-10000/pytorch_model.bin /usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding=&#39;longest&#39;` to pad to the longest sequence in the batch, or use `padding=&#39;max_length&#39;` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert). FutureWarning, Saving model checkpoint to ./results/checkpoint-15000 Configuration saved in ./results/checkpoint-15000/config.json Model weights saved in ./results/checkpoint-15000/pytorch_model.bin /usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding=&#39;longest&#39;` to pad to the longest sequence in the batch, or use `padding=&#39;max_length&#39;` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert). FutureWarning, Saving model checkpoint to ./results/checkpoint-20000 Configuration saved in ./results/checkpoint-20000/config.json Model weights saved in ./results/checkpoint-20000/pytorch_model.bin /usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding=&#39;longest&#39;` to pad to the longest sequence in the batch, or use `padding=&#39;max_length&#39;` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert). FutureWarning, Saving model checkpoint to ./results/checkpoint-25000 Configuration saved in ./results/checkpoint-25000/config.json Model weights saved in ./results/checkpoint-25000/pytorch_model.bin /usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding=&#39;longest&#39;` to pad to the longest sequence in the batch, or use `padding=&#39;max_length&#39;` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert). FutureWarning, Saving model checkpoint to ./results/checkpoint-30000 Configuration saved in ./results/checkpoint-30000/config.json Model weights saved in ./results/checkpoint-30000/pytorch_model.bin /usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding=&#39;longest&#39;` to pad to the longest sequence in the batch, or use `padding=&#39;max_length&#39;` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert). FutureWarning, Saving model checkpoint to ./results/checkpoint-35000 Configuration saved in ./results/checkpoint-35000/config.json Model weights saved in ./results/checkpoint-35000/pytorch_model.bin /usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding=&#39;longest&#39;` to pad to the longest sequence in the batch, or use `padding=&#39;max_length&#39;` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert). FutureWarning, Saving model checkpoint to ./results/checkpoint-40000 Configuration saved in ./results/checkpoint-40000/config.json Model weights saved in ./results/checkpoint-40000/pytorch_model.bin /usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding=&#39;longest&#39;` to pad to the longest sequence in the batch, or use `padding=&#39;max_length&#39;` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert). FutureWarning, Saving model checkpoint to ./results/checkpoint-45000 Configuration saved in ./results/checkpoint-45000/config.json Model weights saved in ./results/checkpoint-45000/pytorch_model.bin /usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding=&#39;longest&#39;` to pad to the longest sequence in the batch, or use `padding=&#39;max_length&#39;` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert). FutureWarning, Saving model checkpoint to ./results/checkpoint-50000 Configuration saved in ./results/checkpoint-50000/config.json Model weights saved in ./results/checkpoint-50000/pytorch_model.bin /usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding=&#39;longest&#39;` to pad to the longest sequence in the batch, or use `padding=&#39;max_length&#39;` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert). FutureWarning, Saving model checkpoint to ./results/checkpoint-55000 Configuration saved in ./results/checkpoint-55000/config.json Model weights saved in ./results/checkpoint-55000/pytorch_model.bin /usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding=&#39;longest&#39;` to pad to the longest sequence in the batch, or use `padding=&#39;max_length&#39;` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert). FutureWarning, Saving model checkpoint to ./results/checkpoint-60000 Configuration saved in ./results/checkpoint-60000/config.json Model weights saved in ./results/checkpoint-60000/pytorch_model.bin /usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding=&#39;longest&#39;` to pad to the longest sequence in the batch, or use `padding=&#39;max_length&#39;` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert). FutureWarning, Training completed. Do not forget to share your model on huggingface.co/models =) . TrainOutput(global_step=63363, training_loss=0.15106624429101714, metrics={&#39;train_runtime&#39;: 16966.2362, &#39;train_samples_per_second&#39;: 59.754, &#39;train_steps_per_second&#39;: 3.735, &#39;total_flos&#39;: 2.048800853818368e+17, &#39;train_loss&#39;: 0.15106624429101714, &#39;epoch&#39;: 3.0}) . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Save model to google drive for later use(evaluation &amp; inference) . from google.colab import drive drive.mount(&#39;/content/gdrive&#39;) . Mounted at /content/gdrive . trainer.save_model(&quot;/content/gdrive/MyDrive/MC_01_03_29_06_2021.bin&quot;) . Saving model checkpoint to /content/gdrive/MyDrive/MC_01_03_29_06_2021.bin Configuration saved in /content/gdrive/MyDrive/MC_01_03_29_06_2021.bin/config.json Model weights saved in /content/gdrive/MyDrive/MC_01_03_29_06_2021.bin/pytorch_model.bin . &lt;/div&gt; .",
            "url": "https://jonathanpro.github.io/myaiblog/jupyter/nlp/tpu/2021/07/07/huggingface_multi_class_pytorch_training.html",
            "relUrl": "/jupyter/nlp/tpu/2021/07/07/huggingface_multi_class_pytorch_training.html",
            "date": " â€¢ Jul 7, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "NLP -  Multi Class Classification - Inference",
            "content": ". from google.colab import drive drive.mount(&#39;/content/gdrive&#39;) . Mounted at /content/gdrive . !pip install transformers . Collecting transformers Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.5MB 30.5MB/s Collecting sacremoses Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 901kB 38.7MB/s Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12) Requirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5) Collecting tokenizers&lt;0.11,&gt;=0.10.1 Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 29.5MB/s Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9) Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0) Requirement already satisfied: importlib-metadata; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20) Collecting huggingface-hub==0.0.12 Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers) (1.15.0) Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers) (7.1.2) Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers) (1.0.1) Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;transformers) (2.4.7) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (1.24.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (2021.5.30) Requirement already satisfied: typing-extensions&gt;=3.6.4; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;transformers) (3.7.4.3) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;transformers) (3.4.1) Installing collected packages: sacremoses, tokenizers, huggingface-hub, transformers Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2 . import pandas as pd import numpy as np import torch import transformers from torch.utils.data import Dataset, DataLoader from transformers import DistilBertModel, DistilBertTokenizer . ! wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip . --2021-07-07 14:56:53-- https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252 Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 29224203 (28M) [application/x-httpd-php] Saving to: â€˜NewsAggregatorDataset.zipâ€™ NewsAggregatorDatas 100%[===================&gt;] 27.87M 23.2MB/s in 1.2s 2021-07-07 14:56:54 (23.2 MB/s) - â€˜NewsAggregatorDataset.zipâ€™ saved [29224203/29224203] . !unzip NewsAggregatorDataset.zip . Archive: NewsAggregatorDataset.zip inflating: 2pageSessions.csv creating: __MACOSX/ inflating: __MACOSX/._2pageSessions.csv inflating: newsCorpora.csv inflating: __MACOSX/._newsCorpora.csv inflating: readme.txt inflating: __MACOSX/._readme.txt . df = pd.read_csv(&#39;newsCorpora.csv&#39;, sep=&#39; t&#39;, names=[&#39;ID&#39;,&#39;TITLE&#39;, &#39;URL&#39;, &#39;PUBLISHER&#39;, &#39;CATEGORY&#39;, &#39;STORY&#39;, &#39;HOSTNAME&#39;, &#39;TIMESTAMP&#39;]) # df.head() # # Removing unwanted columns and only leaving title of news and the category which will be the target df = df[[&#39;TITLE&#39;,&#39;CATEGORY&#39;]] # df.head() # # Converting the codes to appropriate categories using a dictionary my_dict = { &#39;e&#39;:&#39;Entertainment&#39;, &#39;b&#39;:&#39;Business&#39;, &#39;t&#39;:&#39;Science&#39;, &#39;m&#39;:&#39;Health&#39; } def update_cat(x): return my_dict[x] df[&#39;CATEGORY&#39;] = df[&#39;CATEGORY&#39;].apply(lambda x: update_cat(x)) encode_dict = {} def encode_cat(x): if x not in encode_dict.keys(): encode_dict[x]=len(encode_dict) return encode_dict[x] df[&#39;ENCODE_CAT&#39;] = df[&#39;CATEGORY&#39;].apply(lambda x: encode_cat(x)) labels = list(df[&#39;CATEGORY&#39;].unique()) df_mapping=df[[&#39;CATEGORY&#39;, &#39;ENCODE_CAT&#39;]].drop_duplicates() df_mapping = df_mapping.reset_index() id2label = pd.Series(df_mapping.CATEGORY,index=df_mapping.ENCODE_CAT).to_dict() . tokenizer = DistilBertTokenizer.from_pretrained(&#39;distilbert-base-cased&#39;, id2label=id2label) . . class train_data_class(Dataset): def __init__(self, dataframe, tokenizer, max_len): self.len = len(dataframe) self.data = dataframe self.tokenizer = tokenizer self.max_len = max_len def __getitem__(self, index): title = str(self.data.TITLE[index]) title = &quot; &quot;.join(title.split()) inputs = self.tokenizer.encode_plus( title, None, add_special_tokens=True, max_length=self.max_len, pad_to_max_length=True, return_token_type_ids=True, truncation=True ) ids = inputs[&#39;input_ids&#39;] mask = inputs[&#39;attention_mask&#39;] inputs[&#39;label&#39;] = torch.tensor(self.data.ENCODE_CAT[index], dtype=torch.long) del inputs[&#39;token_type_ids&#39;] return inputs &quot;&quot;&quot; return { &#39;ids&#39;: torch.tensor(ids, dtype=torch.long), &#39;mask&#39;: torch.tensor(mask, dtype=torch.long), &#39;label&#39;: torch.tensor(self.data.ENCODE_CAT[index], dtype=torch.long) } &quot;&quot;&quot; def __len__(self): return self.len . MAX_LEN = 512 train_size = 0.8 train_dataset = df.sample(frac=train_size,random_state=200) test_dataset = df.drop(train_dataset.index).reset_index(drop=True) train_dataset = train_dataset.reset_index(drop=True) training_set = train_data_class(train_dataset, tokenizer, MAX_LEN) testing_set = train_data_class(test_dataset, tokenizer, MAX_LEN) . from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments def compute_metrics(pred): labels = pred.label_ids preds = pred.predictions.argmax(-1) precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=&#39;binary&#39;) acc = accuracy_score(labels, preds) return { &#39;accuracy&#39;: acc, &#39;f1&#39;: f1, &#39;precision&#39;: precision, &#39;recall&#39;: recall } training_args = TrainingArguments( output_dir=&#39;./results&#39;, # output directory num_train_epochs=1, # total number of training epochs per_device_train_batch_size=1*16, # batch size per device during training per_device_eval_batch_size=1*64, # batch size for evaluation warmup_steps=500, # number of warmup steps for learning rate scheduler weight_decay=0.01, # strength of weight decay logging_dir=&#39;./logs&#39;, # directory for storing logs logging_steps=1000, ) # save_strategy=&#39;steps&#39;, save_steps=5000 model = AutoModelForSequenceClassification.from_pretrained(&quot;/content/gdrive/MyDrive/MC_03_09_07_05_21.bin&quot;, local_files_only=True, num_labels=len(labels), id2label=id2label) trainer = Trainer( model=model, # the instantiated ðŸ¤— Transformers model to be trained args=training_args, # training arguments, defined above # compute_metrics=compute_metrics, train_dataset=training_set, # training dataset eval_dataset=testing_set # evaluation dataset ) . class data_prediction(torch.utils.data.Dataset): def __init__(self, encodings): self.encodings = encodings def __getitem__(self, idx): item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} return item def __len__(self): return len(self.encodings[&#39;input_ids&#39;]) . def predict_text(text, trainer=trainer, tokenizer=tokenizer): encodings = tokenizer([text], truncation=True, padding=True) dataset = data_prediction(encodings) result = trainer.predict(dataset) return id2label[np.argmax(result.predictions)] . Sample News headline from Google news South Africa . predict_text(&quot;Relieved&#39; Aspen CEO laments impact of US vaccine factory problems on SA &quot;) . ***** Running Prediction ***** Num examples = 1 Batch size = 64 . . [1/1 : &lt; :] &#39;Health&#39; . Go to https://news.google.com/topstories?hl=en-US&amp;gl=US&amp;ceid=US:en and select an english speaking region(e.g. UK, US, New Zealand, AUS, Namibia, etc.) Check the left categories and copy a newline into the &#39;predict_text&#39; function .",
            "url": "https://jonathanpro.github.io/myaiblog/jupyter/nlp/gpu/2021/07/07/huggingface_multi_class_pytorch_inference.html",
            "relUrl": "/jupyter/nlp/gpu/2021/07/07/huggingface_multi_class_pytorch_inference.html",
            "date": " â€¢ Jul 7, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "NLP -  Multi Class Classification - Evaluate",
            "content": ". Usecase: Classify titles of news headline to specific categories . Train model . Load previous trained model . from google.colab import drive drive.mount(&#39;/content/gdrive&#39;) . Mounted at /content/gdrive . !pip install transformers . Collecting transformers Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.5MB 6.6MB/s Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0) Collecting sacremoses Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 901kB 38.1MB/s Requirement already satisfied: importlib-metadata; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0) Requirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5) Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12) Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13) Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20) Collecting huggingface-hub==0.0.12 Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl Collecting tokenizers&lt;0.11,&gt;=0.10.1 Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 24.0MB/s Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;transformers) (2.4.7) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (2021.5.30) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (1.24.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (3.0.4) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers) (1.15.0) Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers) (1.0.1) Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers) (7.1.2) Requirement already satisfied: typing-extensions&gt;=3.6.4; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;transformers) (3.7.4.3) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;transformers) (3.4.1) Installing collected packages: sacremoses, huggingface-hub, tokenizers, transformers Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2 . !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl . Collecting cloud-tpu-client==0.10 Downloading https://files.pythonhosted.org/packages/56/9f/7b1958c2886db06feb5de5b2c191096f9e619914b6c31fdf93999fdbbd8b/cloud_tpu_client-0.10-py3-none-any.whl Collecting torch-xla==1.9 Downloading https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl (149.9MB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 149.9MB 83kB/s Collecting google-api-python-client==1.8.0 Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61kB 3.0MB/s Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (4.1.3) Requirement already satisfied: google-auth-httplib2&gt;=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (0.0.4) Requirement already satisfied: uritemplate&lt;4dev,&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (3.0.1) Requirement already satisfied: google-api-core&lt;2dev,&gt;=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (1.26.3) Requirement already satisfied: six&lt;2dev,&gt;=1.6.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (1.15.0) Requirement already satisfied: google-auth&gt;=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (1.31.0) Requirement already satisfied: httplib2&lt;1dev,&gt;=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (0.17.4) Requirement already satisfied: pyasn1-modules&gt;=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client-&gt;cloud-tpu-client==0.10) (0.2.8) Requirement already satisfied: rsa&gt;=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client-&gt;cloud-tpu-client==0.10) (4.7.2) Requirement already satisfied: pyasn1&gt;=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client-&gt;cloud-tpu-client==0.10) (0.4.8) Requirement already satisfied: packaging&gt;=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (20.9) Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (2018.9) Requirement already satisfied: protobuf&gt;=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (3.12.4) Requirement already satisfied: setuptools&gt;=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (57.0.0) Requirement already satisfied: requests&lt;3.0.0dev,&gt;=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (2.23.0) Requirement already satisfied: googleapis-common-protos&lt;2.0dev,&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (1.53.0) Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth&gt;=1.4.1-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (4.2.2) Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging&gt;=14.3-&gt;google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (2.4.7) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (1.24.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-core&lt;2dev,&gt;=1.13.0-&gt;google-api-python-client==1.8.0-&gt;cloud-tpu-client==0.10) (2021.5.30) ERROR: earthengine-api 0.1.269 has requirement google-api-python-client&lt;2,&gt;=1.12.1, but you&#39;ll have google-api-python-client 1.8.0 which is incompatible. Installing collected packages: google-api-python-client, cloud-tpu-client, torch-xla Found existing installation: google-api-python-client 1.12.8 Uninstalling google-api-python-client-1.12.8: Successfully uninstalled google-api-python-client-1.12.8 Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0 torch-xla-1.9 . import pandas as pd import numpy as np import torch import transformers from torch.utils.data import Dataset, DataLoader from transformers import DistilBertModel, DistilBertTokenizer . ! wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip . --2021-07-06 10:02:06-- https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252 Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 29224203 (28M) [application/x-httpd-php] Saving to: â€˜NewsAggregatorDataset.zipâ€™ NewsAggregatorDatas 100%[===================&gt;] 27.87M 33.3MB/s in 0.8s 2021-07-06 10:02:08 (33.3 MB/s) - â€˜NewsAggregatorDataset.zipâ€™ saved [29224203/29224203] . !unzip NewsAggregatorDataset.zip . Archive: NewsAggregatorDataset.zip inflating: 2pageSessions.csv creating: __MACOSX/ inflating: __MACOSX/._2pageSessions.csv inflating: newsCorpora.csv inflating: __MACOSX/._newsCorpora.csv inflating: readme.txt inflating: __MACOSX/._readme.txt . df = pd.read_csv(&#39;newsCorpora.csv&#39;, sep=&#39; t&#39;, names=[&#39;ID&#39;,&#39;TITLE&#39;, &#39;URL&#39;, &#39;PUBLISHER&#39;, &#39;CATEGORY&#39;, &#39;STORY&#39;, &#39;HOSTNAME&#39;, &#39;TIMESTAMP&#39;]) # df.head() # # Removing unwanted columns and only leaving title of news and the category which will be the target df = df[[&#39;TITLE&#39;,&#39;CATEGORY&#39;]] # df.head() # # Converting the codes to appropriate categories using a dictionary my_dict = { &#39;e&#39;:&#39;Entertainment&#39;, &#39;b&#39;:&#39;Business&#39;, &#39;t&#39;:&#39;Science&#39;, &#39;m&#39;:&#39;Health&#39; } def update_cat(x): return my_dict[x] df[&#39;CATEGORY&#39;] = df[&#39;CATEGORY&#39;].apply(lambda x: update_cat(x)) encode_dict = {} def encode_cat(x): if x not in encode_dict.keys(): encode_dict[x]=len(encode_dict) return encode_dict[x] df[&#39;ENCODE_CAT&#39;] = df[&#39;CATEGORY&#39;].apply(lambda x: encode_cat(x)) labels = list(df[&#39;CATEGORY&#39;].unique()) df_mapping=df[[&#39;CATEGORY&#39;, &#39;ENCODE_CAT&#39;]].drop_duplicates() df_mapping = df_mapping.reset_index() id2label = pd.Series(df_mapping.CATEGORY,index=df_mapping.ENCODE_CAT).to_dict() . tokenizer = DistilBertTokenizer.from_pretrained(&#39;distilbert-base-cased&#39;, id2label=id2label) . . class train_data_class(Dataset): def __init__(self, dataframe, tokenizer, max_len): self.len = len(dataframe) self.data = dataframe self.tokenizer = tokenizer self.max_len = max_len def __getitem__(self, index): title = str(self.data.TITLE[index]) title = &quot; &quot;.join(title.split()) inputs = self.tokenizer.encode_plus( title, None, add_special_tokens=True, max_length=self.max_len, pad_to_max_length=True, return_token_type_ids=True, truncation=True ) ids = inputs[&#39;input_ids&#39;] mask = inputs[&#39;attention_mask&#39;] inputs[&#39;label&#39;] = torch.tensor(self.data.ENCODE_CAT[index], dtype=torch.long) del inputs[&#39;token_type_ids&#39;] return inputs &quot;&quot;&quot; return { &#39;ids&#39;: torch.tensor(ids, dtype=torch.long), &#39;mask&#39;: torch.tensor(mask, dtype=torch.long), &#39;label&#39;: torch.tensor(self.data.ENCODE_CAT[index], dtype=torch.long) } &quot;&quot;&quot; def __len__(self): return self.len . MAX_LEN = 512 train_size = 0.8 train_dataset = df.sample(frac=train_size,random_state=200) test_dataset = df.drop(train_dataset.index).reset_index(drop=True) train_dataset = train_dataset.reset_index(drop=True) training_set = train_data_class(train_dataset, tokenizer, MAX_LEN) testing_set = train_data_class(test_dataset, tokenizer, MAX_LEN) . import torch_xla import torch_xla.core.xla_model as xm device = xm.xla_device() . WARNING:root:Waiting for TPU to be start up with version pytorch-1.9... WARNING:root:Waiting for TPU to be start up with version pytorch-1.9... WARNING:root:TPU has started up successfully with version pytorch-1.9 . from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments from sklearn.metrics import precision_recall_fscore_support, accuracy_score def compute_metrics(pred): labels = pred.label_ids preds = pred.predictions.argmax(-1) acc = accuracy_score(labels, preds) return { &#39;accuracy&#39;: acc } training_args = TrainingArguments( output_dir=&#39;./results&#39;, # output directory num_train_epochs=1, # total number of training epochs per_device_train_batch_size=1*16, # batch size per device during training per_device_eval_batch_size=1*64, # batch size for evaluation warmup_steps=500, # number of warmup steps for learning rate scheduler weight_decay=0.01, # strength of weight decay logging_dir=&#39;./logs&#39;, # directory for storing logs logging_steps=1000, ) # save_strategy=&#39;steps&#39;, save_steps=5000 model = AutoModelForSequenceClassification.from_pretrained(&quot;/content/gdrive/MyDrive/MC_03_09_07_05_21.bin&quot;, local_files_only=True, num_labels=len(labels), id2label=id2label) model = model.to(device) trainer = Trainer( model=model, # the instantiated ðŸ¤— Transformers model to be trained args=training_args, # training arguments, defined above compute_metrics=compute_metrics, train_dataset=training_set, # training dataset eval_dataset=testing_set # evaluation dataset ) . result = trainer.predict(training_set) # np.argmax(result.predictions, axis=1) . ***** Running Prediction ***** Num examples = 337935 Batch size = 64 /usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding=&#39;longest&#39;` to pad to the longest sequence in the batch, or use `padding=&#39;max_length&#39;` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert). FutureWarning, . . [5281/5281 56:34] train_dataset[&#39;ENCODE_PRED&#39;] = np.argmax(result.predictions, axis=1) train_dataset.head() . TITLE CATEGORY ENCODE_CAT ENCODE_PRED . 0 President Barack Obama Releases Statement Abou... | Entertainment | 2 | 2 | . 1 GoPro files to go public | Business | 0 | 0 | . 2 St. Patrick&#39;s Day trivia | Entertainment | 2 | 2 | . 3 Bill Murray Enters the Oscar Race | Entertainment | 2 | 2 | . 4 US safety agency probes older Ram pickup truck... | Science | 1 | 1 | . How to build a confusion matrix . https://deeplizard.com/learn/video/0LhiS6yu2qQ . import matplotlib.pyplot as plt from sklearn.metrics import confusion_matrix cm = confusion_matrix(train_dataset[&#39;ENCODE_CAT&#39;], train_dataset[&#39;ENCODE_PRED&#39;]) import itertools import numpy as np import matplotlib.pyplot as plt def plot_confusion_matrix(cm, classes, normalize=False, title=&#39;Confusion matrix&#39;, cmap=plt.cm.Blues): if normalize: cm = cm.astype(&#39;float&#39;) / cm.sum(axis=1)[:, np.newaxis] print(&quot;Normalized confusion matrix&quot;) else: print(&#39;Confusion matrix, without normalization&#39;) print(cm) plt.imshow(cm, interpolation=&#39;nearest&#39;, cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes) fmt = &#39;.2f&#39; if normalize else &#39;d&#39; thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=&quot;center&quot;, color=&quot;white&quot; if cm[i, j] &gt; thresh else &quot;black&quot;) plt.tight_layout() plt.ylabel(&#39;True label&#39;) plt.xlabel(&#39;Predicted label&#39;) plt.figure(figsize=(10,10)) plot_confusion_matrix(cm, labels) . Confusion matrix, without normalization [[ 92212 371 99 70] [ 546 85887 72 26] [ 98 62 121905 27] [ 106 34 60 36360]] . TODO explanation for confusion matrix . trainer.evaluate() . ***** Running Evaluation ***** Num examples = 84484 Batch size = 64 /usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding=&#39;longest&#39;` to pad to the longest sequence in the batch, or use `padding=&#39;max_length&#39;` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert). FutureWarning, . . [5281/5281 1:10:47] {&#39;eval_accuracy&#39;: 0.9565834950996639, &#39;eval_loss&#39;: 0.2912846505641937, &#39;eval_runtime&#39;: 851.0253, &#39;eval_samples_per_second&#39;: 99.273, &#39;eval_steps_per_second&#39;: 1.552} . TODO calcluate &amp; interpre results .",
            "url": "https://jonathanpro.github.io/myaiblog/jupyter/nlp/tpu/2021/07/07/huggingface_multi_class_pytorch_evaluate.html",
            "relUrl": "/jupyter/nlp/tpu/2021/07/07/huggingface_multi_class_pytorch_evaluate.html",
            "date": " â€¢ Jul 7, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "NLP -  Binary Class Classification",
            "content": ". !pip install transformers . Collecting transformers Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.3MB 7.4MB/s Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9) Requirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5) Collecting tokenizers&lt;0.11,&gt;=0.10.1 Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 51.2MB/s Requirement already satisfied: importlib-metadata; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1) Collecting sacremoses Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 901kB 45.3MB/s Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12) Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1) Collecting huggingface-hub==0.0.8 Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20) Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;transformers) (2.4.7) Requirement already satisfied: typing-extensions&gt;=3.6.4; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;transformers) (3.7.4.3) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;transformers) (3.4.1) Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers) (7.1.2) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers) (1.15.0) Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers) (1.0.1) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (3.0.4) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (1.24.3) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (2020.12.5) Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1 . !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz !tar -xf aclImdb_v1.tar.gz . --2021-06-09 09:26:12-- http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10 Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 84125825 (80M) [application/x-gzip] Saving to: â€˜aclImdb_v1.tar.gzâ€™ aclImdb_v1.tar.gz 100%[===================&gt;] 80.23M 17.1MB/s in 4.2s 2021-06-09 09:26:16 (19.1 MB/s) - â€˜aclImdb_v1.tar.gzâ€™ saved [84125825/84125825] . !ls . aclImdb aclImdb_v1.tar.gz sample_data . from pathlib import Path def read_imdb_split(split_dir): split_dir = Path(split_dir) texts = [] labels = [] for label_dir in [&quot;pos&quot;, &quot;neg&quot;]: for text_file in (split_dir/label_dir).iterdir(): texts.append(text_file.read_text()) labels.append(0 if label_dir is &quot;neg&quot; else 1) return texts, labels train_texts, train_labels = read_imdb_split(&#39;aclImdb/train&#39;) test_texts, test_labels = read_imdb_split(&#39;aclImdb/test&#39;) . from sklearn.model_selection import train_test_split train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2) . len(train_texts) . 20000 . from transformers import DistilBertTokenizerFast tokenizer = DistilBertTokenizerFast.from_pretrained(&#39;distilbert-base-uncased&#39;) . . train_encodings = tokenizer(train_texts, truncation=True, padding=True) val_encodings = tokenizer(val_texts, truncation=True, padding=True) test_encodings = tokenizer(test_texts, truncation=True, padding=True) . import torch class IMDbDataset(torch.utils.data.Dataset): def __init__(self, encodings, labels): self.encodings = encodings self.labels = labels def __getitem__(self, idx): item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} item[&#39;labels&#39;] = torch.tensor(self.labels[idx]) return item def __len__(self): return len(self.labels) train_dataset = IMDbDataset(train_encodings, train_labels) val_dataset = IMDbDataset(val_encodings, val_labels) test_dataset = IMDbDataset(test_encodings, test_labels) . from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments training_args = TrainingArguments( output_dir=&#39;./results&#39;, # output directory num_train_epochs=3, # total number of training epochs per_device_train_batch_size=16, # batch size per device during training per_device_eval_batch_size=64, # batch size for evaluation warmup_steps=500, # number of warmup steps for learning rate scheduler weight_decay=0.01, # strength of weight decay logging_dir=&#39;./logs&#39;, # directory for storing logs logging_steps=10, ) model = DistilBertForSequenceClassification.from_pretrained(&quot;distilbert-base-uncased&quot;) trainer = Trainer( model=model, # the instantiated ðŸ¤— Transformers model to be trained args=training_args, # training arguments, defined above train_dataset=train_dataset, # training dataset eval_dataset=val_dataset # evaluation dataset ) trainer.train() . . Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: [&#39;vocab_layer_norm.weight&#39;, &#39;vocab_projector.bias&#39;, &#39;vocab_transform.weight&#39;, &#39;vocab_transform.bias&#39;, &#39;vocab_projector.weight&#39;, &#39;vocab_layer_norm.bias&#39;] - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: [&#39;pre_classifier.bias&#39;, &#39;pre_classifier.weight&#39;, &#39;classifier.bias&#39;, &#39;classifier.weight&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . . [3750/3750 52:46, Epoch 3/3] Step Training Loss . 10 | 0.684600 | . 20 | 0.691600 | . 30 | 0.691300 | . 40 | 0.695900 | . 50 | 0.682900 | . 60 | 0.676900 | . 70 | 0.665900 | . 80 | 0.645700 | . 90 | 0.602600 | . 100 | 0.496700 | . 110 | 0.429800 | . 120 | 0.340100 | . 130 | 0.327300 | . 140 | 0.306200 | . 150 | 0.333500 | . 160 | 0.259100 | . 170 | 0.254000 | . 180 | 0.438300 | . 190 | 0.333900 | . 200 | 0.362500 | . 210 | 0.312800 | . 220 | 0.398800 | . 230 | 0.377000 | . 240 | 0.326000 | . 250 | 0.243200 | . 260 | 0.327100 | . 270 | 0.290200 | . 280 | 0.304400 | . 290 | 0.197300 | . 300 | 0.319200 | . 310 | 0.365600 | . 320 | 0.310700 | . 330 | 0.279300 | . 340 | 0.452500 | . 350 | 0.289900 | . 360 | 0.341700 | . 370 | 0.397100 | . 380 | 0.327000 | . 390 | 0.196400 | . 400 | 0.171100 | . 410 | 0.362800 | . 420 | 0.247400 | . 430 | 0.251200 | . 440 | 0.288100 | . 450 | 0.359200 | . 460 | 0.308200 | . 470 | 0.253800 | . 480 | 0.234300 | . 490 | 0.196900 | . 500 | 0.358900 | . 510 | 0.288100 | . 520 | 0.400000 | . 530 | 0.294000 | . 540 | 0.356800 | . 550 | 0.301600 | . 560 | 0.339900 | . 570 | 0.263500 | . 580 | 0.270900 | . 590 | 0.320400 | . 600 | 0.294500 | . 610 | 0.323100 | . 620 | 0.234900 | . 630 | 0.267100 | . 640 | 0.328700 | . 650 | 0.313700 | . 660 | 0.224200 | . 670 | 0.284300 | . 680 | 0.210900 | . 690 | 0.404200 | . 700 | 0.301900 | . 710 | 0.219300 | . 720 | 0.167300 | . 730 | 0.514900 | . 740 | 0.236200 | . 750 | 0.241200 | . 760 | 0.344900 | . 770 | 0.217600 | . 780 | 0.327000 | . 790 | 0.206400 | . 800 | 0.250900 | . 810 | 0.289100 | . 820 | 0.344400 | . 830 | 0.221300 | . 840 | 0.238800 | . 850 | 0.321800 | . 860 | 0.191700 | . 870 | 0.266200 | . 880 | 0.211200 | . 890 | 0.338400 | . 900 | 0.201400 | . 910 | 0.305000 | . 920 | 0.352600 | . 930 | 0.352000 | . 940 | 0.254900 | . 950 | 0.301100 | . 960 | 0.256400 | . 970 | 0.230800 | . 980 | 0.281700 | . 990 | 0.147900 | . 1000 | 0.184400 | . 1010 | 0.267400 | . 1020 | 0.343800 | . 1030 | 0.271500 | . 1040 | 0.280900 | . 1050 | 0.334700 | . 1060 | 0.242200 | . 1070 | 0.307200 | . 1080 | 0.162500 | . 1090 | 0.329900 | . 1100 | 0.193200 | . 1110 | 0.130000 | . 1120 | 0.278000 | . 1130 | 0.208800 | . 1140 | 0.283600 | . 1150 | 0.233200 | . 1160 | 0.289100 | . 1170 | 0.236900 | . 1180 | 0.226100 | . 1190 | 0.325300 | . 1200 | 0.210300 | . 1210 | 0.140000 | . 1220 | 0.170800 | . 1230 | 0.325700 | . 1240 | 0.507100 | . 1250 | 0.293300 | . 1260 | 0.157200 | . 1270 | 0.156000 | . 1280 | 0.176100 | . 1290 | 0.226800 | . 1300 | 0.217500 | . 1310 | 0.216000 | . 1320 | 0.061400 | . 1330 | 0.237400 | . 1340 | 0.204700 | . 1350 | 0.173500 | . 1360 | 0.135200 | . 1370 | 0.149600 | . 1380 | 0.241000 | . 1390 | 0.083100 | . 1400 | 0.218800 | . 1410 | 0.164600 | . 1420 | 0.182200 | . 1430 | 0.178200 | . 1440 | 0.162000 | . 1450 | 0.187100 | . 1460 | 0.162400 | . 1470 | 0.143100 | . 1480 | 0.092100 | . 1490 | 0.191700 | . 1500 | 0.166300 | . 1510 | 0.176500 | . 1520 | 0.048800 | . 1530 | 0.200000 | . 1540 | 0.264300 | . 1550 | 0.214900 | . 1560 | 0.213700 | . 1570 | 0.201900 | . 1580 | 0.120200 | . 1590 | 0.153100 | . 1600 | 0.119000 | . 1610 | 0.064000 | . 1620 | 0.202000 | . 1630 | 0.200400 | . 1640 | 0.183400 | . 1650 | 0.138800 | . 1660 | 0.184100 | . 1670 | 0.193900 | . 1680 | 0.169300 | . 1690 | 0.248700 | . 1700 | 0.124800 | . 1710 | 0.158200 | . 1720 | 0.162700 | . 1730 | 0.137700 | . 1740 | 0.197700 | . 1750 | 0.277100 | . 1760 | 0.181900 | . 1770 | 0.194900 | . 1780 | 0.091900 | . 1790 | 0.103100 | . 1800 | 0.154300 | . 1810 | 0.205300 | . 1820 | 0.121500 | . 1830 | 0.160700 | . 1840 | 0.115800 | . 1850 | 0.065800 | . 1860 | 0.052200 | . 1870 | 0.160700 | . 1880 | 0.344100 | . 1890 | 0.183000 | . 1900 | 0.160600 | . 1910 | 0.126300 | . 1920 | 0.233100 | . 1930 | 0.178100 | . 1940 | 0.165200 | . 1950 | 0.211100 | . 1960 | 0.174300 | . 1970 | 0.135600 | . 1980 | 0.096800 | . 1990 | 0.206400 | . 2000 | 0.176700 | . 2010 | 0.201100 | . 2020 | 0.128200 | . 2030 | 0.110000 | . 2040 | 0.147100 | . 2050 | 0.188600 | . 2060 | 0.144200 | . 2070 | 0.096700 | . 2080 | 0.223200 | . 2090 | 0.196800 | . 2100 | 0.143400 | . 2110 | 0.208100 | . 2120 | 0.143000 | . 2130 | 0.301800 | . 2140 | 0.055300 | . 2150 | 0.091900 | . 2160 | 0.085400 | . 2170 | 0.136600 | . 2180 | 0.136500 | . 2190 | 0.142700 | . 2200 | 0.148400 | . 2210 | 0.234800 | . 2220 | 0.149900 | . 2230 | 0.176500 | . 2240 | 0.117600 | . 2250 | 0.153700 | . 2260 | 0.213200 | . 2270 | 0.174500 | . 2280 | 0.108400 | . 2290 | 0.231100 | . 2300 | 0.143000 | . 2310 | 0.187700 | . 2320 | 0.123700 | . 2330 | 0.228600 | . 2340 | 0.221700 | . 2350 | 0.176000 | . 2360 | 0.072700 | . 2370 | 0.122600 | . 2380 | 0.149600 | . 2390 | 0.068600 | . 2400 | 0.228700 | . 2410 | 0.222100 | . 2420 | 0.176600 | . 2430 | 0.082100 | . 2440 | 0.128300 | . 2450 | 0.135700 | . 2460 | 0.253200 | . 2470 | 0.101000 | . 2480 | 0.106300 | . 2490 | 0.136600 | . 2500 | 0.221200 | . 2510 | 0.051900 | . 2520 | 0.089800 | . 2530 | 0.006800 | . 2540 | 0.038600 | . 2550 | 0.079200 | . 2560 | 0.096100 | . 2570 | 0.072300 | . 2580 | 0.061700 | . 2590 | 0.048900 | . 2600 | 0.137700 | . 2610 | 0.042400 | . 2620 | 0.066000 | . 2630 | 0.099800 | . 2640 | 0.020200 | . 2650 | 0.088900 | . 2660 | 0.026900 | . 2670 | 0.016700 | . 2680 | 0.046000 | . 2690 | 0.007000 | . 2700 | 0.041000 | . 2710 | 0.040800 | . 2720 | 0.009800 | . 2730 | 0.129800 | . 2740 | 0.062100 | . 2750 | 0.051000 | . 2760 | 0.078500 | . 2770 | 0.154900 | . 2780 | 0.108700 | . 2790 | 0.019400 | . 2800 | 0.010500 | . 2810 | 0.086800 | . 2820 | 0.087700 | . 2830 | 0.003200 | . 2840 | 0.120600 | . 2850 | 0.093000 | . 2860 | 0.089100 | . 2870 | 0.076800 | . 2880 | 0.044200 | . 2890 | 0.035600 | . 2900 | 0.093000 | . 2910 | 0.074100 | . 2920 | 0.119100 | . 2930 | 0.034900 | . 2940 | 0.052400 | . 2950 | 0.061000 | . 2960 | 0.113700 | . 2970 | 0.063500 | . 2980 | 0.142200 | . 2990 | 0.109800 | . 3000 | 0.004200 | . 3010 | 0.050300 | . 3020 | 0.081100 | . 3030 | 0.077100 | . 3040 | 0.128600 | . 3050 | 0.053000 | . 3060 | 0.058900 | . 3070 | 0.059500 | . 3080 | 0.103800 | . 3090 | 0.038100 | . 3100 | 0.108300 | . 3110 | 0.047000 | . 3120 | 0.121300 | . 3130 | 0.056100 | . 3140 | 0.024500 | . 3150 | 0.059300 | . 3160 | 0.042900 | . 3170 | 0.043700 | . 3180 | 0.082900 | . 3190 | 0.113600 | . 3200 | 0.082800 | . 3210 | 0.061400 | . 3220 | 0.059800 | . 3230 | 0.073900 | . 3240 | 0.110200 | . 3250 | 0.101800 | . 3260 | 0.061700 | . 3270 | 0.068800 | . 3280 | 0.117900 | . 3290 | 0.068800 | . 3300 | 0.044800 | . 3310 | 0.046700 | . 3320 | 0.053500 | . 3330 | 0.081100 | . 3340 | 0.120800 | . 3350 | 0.077300 | . 3360 | 0.006200 | . 3370 | 0.073700 | . 3380 | 0.058300 | . 3390 | 0.073400 | . 3400 | 0.042600 | . 3410 | 0.069600 | . 3420 | 0.052400 | . 3430 | 0.081100 | . 3440 | 0.085000 | . 3450 | 0.012800 | . 3460 | 0.137500 | . 3470 | 0.066000 | . 3480 | 0.159700 | . 3490 | 0.038100 | . 3500 | 0.007700 | . 3510 | 0.091100 | . 3520 | 0.095400 | . 3530 | 0.044600 | . 3540 | 0.104700 | . 3550 | 0.047300 | . 3560 | 0.004200 | . 3570 | 0.029700 | . 3580 | 0.062100 | . 3590 | 0.108100 | . 3600 | 0.114800 | . 3610 | 0.043000 | . 3620 | 0.169600 | . 3630 | 0.002600 | . 3640 | 0.046200 | . 3650 | 0.096100 | . 3660 | 0.056900 | . 3670 | 0.008400 | . 3680 | 0.051300 | . 3690 | 0.044100 | . 3700 | 0.151900 | . 3710 | 0.095900 | . 3720 | 0.078200 | . 3730 | 0.003900 | . 3740 | 0.042200 | . 3750 | 0.046800 | . &lt;/div&gt; &lt;/div&gt; TrainOutput(global_step=3750, training_loss=0.18305039387245972, metrics={&#39;train_runtime&#39;: 3167.2396, &#39;train_samples_per_second&#39;: 1.184, &#39;total_flos&#39;: 822743162880000.0, &#39;epoch&#39;: 3.0, &#39;init_mem_cpu_alloc_delta&#39;: 2495520768, &#39;init_mem_gpu_alloc_delta&#39;: 268953088, &#39;init_mem_cpu_peaked_delta&#39;: 0, &#39;init_mem_gpu_peaked_delta&#39;: 0, &#39;train_mem_cpu_alloc_delta&#39;: -84152320, &#39;train_mem_gpu_alloc_delta&#39;: 804118016, &#39;train_mem_cpu_peaked_delta&#39;: 192946176, &#39;train_mem_gpu_peaked_delta&#39;: 6566123008}) . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; class IMDbDataset_pre(torch.utils.data.Dataset): def __init__(self, encodings): self.encodings = encodings def __getitem__(self, idx): item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} return item def __len__(self): return len(self.encodings[&#39;input_ids&#39;]) . import numpy as np def predict_text(text, trainer=trainer, tokenizer=tokenizer): encodings = tokenizer([text], truncation=True, padding=True) dataset = IMDbDataset_pre(encodings) result = trainer.predict(dataset) return bool(np.argmax(result.predictions,axis=1)[0]) . predict_text(&quot;This movie is fucking shit&quot;, trainer) . . [1/1 00:22] False . predict_text(&quot;This movie is fucking awesome&quot;, trainer) . . [1/1 00:27] True . &lt;/div&gt; .",
            "url": "https://jonathanpro.github.io/myaiblog/jupyter/nlp/gpu/2021/07/04/huggingface_bi_class_pytorch.html",
            "relUrl": "/jupyter/nlp/gpu/2021/07/04/huggingface_bi_class_pytorch.html",
            "date": " â€¢ Jul 4, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.â†© . 2. This is the other footnote. You can even have a link!â†© .",
            "url": "https://jonathanpro.github.io/myaiblog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " â€¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a â€œlevel 1 headingâ€ in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Hereâ€™s a footnote 1. Hereâ€™s a horizontal rule: . . Lists . Hereâ€™s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes â€¦andâ€¦ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.Â &#8617; . |",
            "url": "https://jonathanpro.github.io/myaiblog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " â€¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.Â &#8617; . |",
          "url": "https://jonathanpro.github.io/myaiblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://jonathanpro.github.io/myaiblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}